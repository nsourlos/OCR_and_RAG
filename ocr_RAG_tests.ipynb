{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f486a227",
   "metadata": {},
   "source": [
    "# OCR and RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f0cbbe7",
   "metadata": {},
   "source": [
    "[RAG on PDF from OpenAI](https://cookbook.openai.com/examples/parse_pdf_docs_for_rag) - Best results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d98dc79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from pdf2image import convert_from_path\n",
    "from pdfminer.high_level import extract_text\n",
    "import base64\n",
    "import io\n",
    "import os\n",
    "import concurrent.futures\n",
    "from tqdm import tqdm\n",
    "from openai import OpenAI\n",
    "import re\n",
    "import json\n",
    "from rich import print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a0e5d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "# Load environment variables from .env file\n",
    "load_dotenv(dotenv_path=os.getcwd()+\"/env\")\n",
    "\n",
    "# Get the OpenAI API key\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY_DRACO')\n",
    "# Initializing OpenAI client - see https://platform.openai.com/docs/quickstart?context=python\n",
    "client = OpenAI(api_key=openai_api_key)\n",
    "openai_model=\"gpt-4o\"\n",
    "openai_small_model='gpt-4o-mini'\n",
    "\n",
    "files_path = \"pdfs\"\n",
    "pdf_file=\"HardyCross.pdf\"\n",
    "contract='Contract.pdf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f64a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the cleaned content to a file\n",
    "desktop_path = os.path.join(os.path.expanduser(\"~\"), \"Desktop\")\n",
    "file_path_clean = os.path.join(desktop_path, \"cleaned_content.md\")\n",
    "\n",
    "file_path_docling=os.path.join(desktop_path, \"docling.md\")\n",
    "file_path_pytesseract=os.path.join(desktop_path, \"pytesseract.md\")\n",
    "file_path_mistral_ocr=os.path.join(desktop_path, \"mistral_ocr.md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c69e8ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_doc_to_images(path):\n",
    "    images = convert_from_path(path)\n",
    "    return images\n",
    "\n",
    "def extract_text_from_doc(path):\n",
    "    text = extract_text(path)\n",
    "    return text\n",
    "\n",
    "# Converting images to base64 encoded images in a data URI format to use with the ChatCompletions API\n",
    "def get_img_uri(img):\n",
    "    png_buffer = io.BytesIO()\n",
    "    img.save(png_buffer, format=\"PNG\")\n",
    "    png_buffer.seek(0)\n",
    "\n",
    "    base64_png = base64.b64encode(png_buffer.read()).decode('utf-8')\n",
    "\n",
    "    data_uri = f\"data:image/png;base64,{base64_png}\"\n",
    "    return data_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc394234",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_doc_image(img):\n",
    "    img_uri = get_img_uri(img)\n",
    "    data = analyze_image(img_uri)\n",
    "    return data\n",
    "\n",
    "\n",
    "def analyze_image(data_uri):\n",
    "    response = client.chat.completions.create(\n",
    "        model=openai_model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\": {\n",
    "                        \"url\": f\"{data_uri}\"\n",
    "                    }\n",
    "                    }\n",
    "                ]\n",
    "                },\n",
    "        ],\n",
    "        max_tokens=500,\n",
    "        temperature=0,\n",
    "        top_p=0.1\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1c69913b",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt=\"\"\"\n",
    "Parse with OCR the contents of this page extracted from a PDF file. Include all the information, including equations in markdown format. \n",
    "If there is a latex equation, convert it to markdown format by including it between $$\n",
    "\n",
    "# Describe visual elements in detail (but include all the information and text of the original image as is):\n",
    "\n",
    "# - **Diagrams**: Explain each component and how they interact. For example, \"The process begins with X, which then leads to Y and results in Z.\". \n",
    "Include relevant information such as arrays that show direction.\n",
    "  \n",
    "# - **Tables**: Break down the information logically. For instance, \"Product A costs X dollars, while Product B is priced at Y dollars.\"\n",
    "\n",
    "# Focus on the content itself rather than the format:\n",
    "\n",
    "# - **DO NOT** include terms referring to the content format.\n",
    "  \n",
    "# - **DO NOT** mention the content type. Instead, directly discuss the information presented.\n",
    "\n",
    "# Keep your explanation comprehensive yet concise:\n",
    "\n",
    "# - Be exhaustive in describing the content, as your audience cannot see the image.\n",
    "  \n",
    "# - Exclude irrelevant details such as page numbers or the position of elements on the image.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f300e765",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_items = os.listdir(files_path)\n",
    "files = [item for item in all_items if os.path.isfile(os.path.join(files_path, item)) and item.endswith(('.pdf', '.doc', '.docx', '.txt'))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b25f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = []\n",
    "\n",
    "for f in files:\n",
    "    \n",
    "    path = f\"{files_path}/{f}\"\n",
    "    doc = {\n",
    "        \"filename\": f\n",
    "    }\n",
    "    text = extract_text_from_doc(path)\n",
    "    doc['text'] = text\n",
    "    imgs = convert_doc_to_images(path)\n",
    "    pages_description = []\n",
    "    \n",
    "    print(f\"Analyzing pages for doc {f}\")\n",
    "    \n",
    "    # Concurrent execution\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=8) as executor:\n",
    "        \n",
    "        # Removing 1st slide as it's usually just an intro\n",
    "        futures = [\n",
    "            executor.submit(analyze_doc_image, img)\n",
    "            for img in imgs[:]\n",
    "        ]\n",
    "        \n",
    "        with tqdm(total=len(imgs)) as pbar:\n",
    "            for _ in concurrent.futures.as_completed(futures):\n",
    "                pbar.update(1)\n",
    "        \n",
    "        for f in futures:\n",
    "            res = f.result()\n",
    "            pages_description.append(res)\n",
    "        \n",
    "    doc['pages_description'] = pages_description\n",
    "    docs.append(doc)\n",
    "\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c250dc7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunking content by page and merging together slides text & description if applicable\n",
    "content = []\n",
    "for doc in docs:\n",
    "    # Removing first slide as well\n",
    "    text = doc['text'].split('\\f')[:]\n",
    "    description = doc['pages_description']\n",
    "    description_indexes = []\n",
    "    for i in range(len(text)):\n",
    "        slide_content = text[i] + '\\n'\n",
    "        # Trying to find matching slide description\n",
    "        slide_title = text[i].split('\\n')[0]\n",
    "        for j in range(len(description)):\n",
    "            description_title = description[j].split('\\n')[0]\n",
    "            if slide_title.lower() == description_title.lower():\n",
    "                slide_content += description[j].replace(description_title, '')\n",
    "                # Keeping track of the descriptions added\n",
    "                description_indexes.append(j)\n",
    "        # Adding the slide content + matching slide description to the content pieces\n",
    "        content.append(slide_content) \n",
    "    # Adding the slides descriptions that weren't used\n",
    "    for j in range(len(description)):\n",
    "        if j not in description_indexes:\n",
    "            content.append(description[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "acb98e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning up content\n",
    "# Removing trailing spaces, additional line breaks, page numbers and references to the content being a slide\n",
    "clean_content = []\n",
    "for c in content:\n",
    "    text = c.replace(' \\n', '').replace('\\n\\n', '\\n').replace('\\n\\n\\n', '\\n').strip()\n",
    "    text = re.sub(r\"(?<=\\n)\\d{1,2}\", \"\", text)\n",
    "    text = re.sub(r\"\\b(?:the|this)\\s*slide\\s*\\w+\\b\", \"\", text, flags=re.IGNORECASE)\n",
    "    clean_content.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "313e3f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_string = ''.join(clean_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "56205f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_prompt=f\"\"\"\n",
    "In the following text the equations that should be shown in an markdown file are not rendered properly. Return a version where the errors in formatting are fixed and the equations \n",
    "can be displayed properly. Convert latex block equations to markdown version by including them between $$...$$. For inline math use $...$.\n",
    "Return only the content that should be directly copy pasted in a md file. Do not return it in quotes, just plain code.\n",
    "Text: {merged_string}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ed77c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": clean_prompt}\n",
    "]\n",
    "\n",
    "# Use OpenAI to judge tool usage\n",
    "import openai\n",
    "from langsmith.wrappers import wrap_openai\n",
    "client = wrap_openai(openai.Client(api_key=openai_api_key))\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    messages=messages,\n",
    "    temperature=0,\n",
    "    model=openai_small_model,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "answer = response.choices[0].message.content.strip()\n",
    "\n",
    "with open(file_path_clean, \"w\") as f:\n",
    "    f.write(answer)\n",
    "\n",
    "# Saving result to file for later\n",
    "json_path = \"parsed_pdf_docs.json\"\n",
    "\n",
    "with open(json_path, 'w') as f:\n",
    "    json.dump(docs, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c2b7bcb",
   "metadata": {},
   "source": [
    "OCR tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e566d25",
   "metadata": {},
   "source": [
    "[Marked-pdf](https://github.com/VikParuchuri/marker) - very good!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87574f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "from marker.converters.pdf import PdfConverter\n",
    "from marker.models import create_model_dict\n",
    "from marker.output import text_from_rendered\n",
    "\n",
    "converter = PdfConverter(\n",
    "    artifact_dict=create_model_dict(),\n",
    ")\n",
    "rendered = converter(pdf_file)\n",
    "text, _, images = text_from_rendered(rendered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "298073a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a82d19e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252f1eb0",
   "metadata": {},
   "source": [
    "If we wanted to change the path and display imgs in Markdown file too - Markdown inside Jupyter cells doesn't not support that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f4ea16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os, re\n",
    "# from IPython.display import display, Markdown\n",
    "\n",
    "# def display_hardy_cross_with_images(markdown_text, image_dict=None, output_dir='/img'):\n",
    "#     if image_dict is None:\n",
    "#         image_dict = {}\n",
    "\n",
    "#     os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "#     def replace_image(match):\n",
    "#         image_path = match.group(1)\n",
    "#         image_name = os.path.basename(image_path)\n",
    "\n",
    "#         if image_name in image_dict:\n",
    "#             save_path = os.path.join(output_dir, image_name)\n",
    "#             image_dict[image_name].save(save_path)\n",
    "#             return f\" ![{image_name}]({save_path})\"\n",
    "#         else:\n",
    "#             return f\"[Image: {image_name} not found]\"\n",
    "\n",
    "#     processed_text = re.sub(r'!\\[\\]\\(([^)]+)\\)', replace_image, markdown_text)\n",
    "#     display(Markdown(processed_text))\n",
    "#     return processed_text\n",
    "\n",
    "# display_hardy_cross_with_images(rendered.markdown, image_dict=images)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb74402",
   "metadata": {},
   "source": [
    "[Docling](https://github.com/docling-project/docling) (ok for contract, but not good for pdf with images and/or equations) - install with ```pip install docling```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a5be446",
   "metadata": {},
   "outputs": [],
   "source": [
    "from docling.document_converter import DocumentConverter\n",
    "\n",
    "# source = \"H\"  # document per local path or URL\n",
    "converter = DocumentConverter()\n",
    "result = converter.convert(contract)\n",
    "\n",
    "merged_string = result.document.export_to_markdown()\n",
    "\n",
    "clean_prompt=f\"\"\"\n",
    "Translate the following text from Dutch to English\n",
    "Text: {merged_string}\n",
    "\"\"\"\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": clean_prompt}\n",
    "]\n",
    "\n",
    "# Use OpenAI to judge tool usage\n",
    "import openai\n",
    "from langsmith.wrappers import wrap_openai\n",
    "client = wrap_openai(openai.Client(api_key=openai_api_key))\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    messages=messages,\n",
    "    temperature=0,\n",
    "    model=openai_small_model,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "answer = response.choices[0].message.content.strip()\n",
    "\n",
    "with open(file_path_docling, \"w\") as f:\n",
    "    f.write(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad18d06d",
   "metadata": {},
   "source": [
    "Pytesseract - Not good for equations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ceb47287",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytesseract\n",
    "from pdf2image import convert_from_path\n",
    "\n",
    "images = convert_from_path(pdf_file)\n",
    "ocr_text = \"\"\n",
    "\n",
    "for img in images:\n",
    "    ocr_text += pytesseract.image_to_string(img, lang=\"eng\") + \"\\n\\n\"\n",
    "\n",
    "# Display the first few lines as a preview\n",
    "import markdownify\n",
    "\n",
    "markdown_text = markdownify.markdownify(ocr_text)\n",
    "\n",
    "with open(file_path_pytesseract, \"w\") as f:\n",
    "    f.write(markdown_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f8fdc6c",
   "metadata": {},
   "source": [
    "[Mistral OCR](https://docs.mistral.ai/capabilities/document/) (really bad for equations, ok for contract but still errors in format and gpt4o better) - install with ```pip install mistralai```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7cdbd944",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from mistralai import Mistral\n",
    "\n",
    "mistral_api_key = os.getenv('MISTRAL_API_KEY')\n",
    "\n",
    "client = Mistral(api_key=mistral_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "67bae53f",
   "metadata": {},
   "outputs": [],
   "source": [
    "uploaded_pdf = client.files.upload(\n",
    "    file={\n",
    "        \"file_name\": contract,\n",
    "        \"content\": open(contract, \"rb\"),\n",
    "    },\n",
    "    purpose=\"ocr\"\n",
    ")  \n",
    "\n",
    "signed_url = client.files.get_signed_url(file_id=uploaded_pdf.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4b000c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "ocr_response = client.ocr.process(\n",
    "    model=\"mistral-ocr-latest\",\n",
    "    document={\n",
    "        \"type\": \"document_url\",\n",
    "        \"document_url\": signed_url.url,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f04ac52f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first few lines as a preview\n",
    "import markdownify\n",
    "markdown_text = markdownify.markdownify(ocr_response.pages[0].markdown)\n",
    "\n",
    "with open(file_path_mistral_ocr, \"w\") as f:\n",
    "    f.write(markdown_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5a6f4467",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # For an online doc\n",
    "# ocr_response = client.ocr.process(\n",
    "#     model=\"mistral-ocr-latest\",\n",
    "#     document={\n",
    "#         \"type\": \"document_url\",\n",
    "#         \"document_url\": \"https://arxiv.org/pdf/2201.04234\"\n",
    "#     },\n",
    "#     include_image_base64=True\n",
    "# )\n",
    "\n",
    "# # For an online image\n",
    "# ocr_response = client.ocr.process(\n",
    "#     model=\"mistral-ocr-latest\",\n",
    "#     document={\n",
    "#         \"type\": \"image_url\",\n",
    "#         \"image_url\": \"https://raw.githubusercontent.com/mistralai/cookbook/refs/heads/main/mistral/ocr/receipt.png\"\n",
    "#     }\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a14ee87",
   "metadata": {},
   "source": [
    "[surya-ocr](https://github.com/VikParuchuri/surya) - not that good, fails for equations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "439b2636",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from surya.recognition import RecognitionPredictor\n",
    "from surya.detection import DetectionPredictor\n",
    "\n",
    "image = Image.open(\"test.png\") #img extracted from HardyCross.pdf with equations\n",
    "langs = [\"en\"] # Replace with your languages or pass None (recommended to use None)\n",
    "recognition_predictor = RecognitionPredictor()\n",
    "detection_predictor = DetectionPredictor()\n",
    "\n",
    "predictions = recognition_predictor([image], [langs], detection_predictor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b007e803",
   "metadata": {},
   "outputs": [],
   "source": [
    "[x.text for x in predictions[0].text_lines]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0637f817",
   "metadata": {},
   "source": [
    "[MGP Alibaba](https://huggingface.co/alibaba-damo/mgp-str-base) - very bad, just one word output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ac5cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import MgpstrProcessor, MgpstrForSceneTextRecognition\n",
    "import requests\n",
    "from PIL import Image\n",
    "\n",
    "processor = MgpstrProcessor.from_pretrained('alibaba-damo/mgp-str-base')\n",
    "model = MgpstrForSceneTextRecognition.from_pretrained('alibaba-damo/mgp-str-base')\n",
    "\n",
    "# load image from the IIIT-5k dataset\n",
    "# url = \"https://i.postimg.cc/ZKwLg2Gw/367-14.png\"\n",
    "# image = Image.open(requests.get(url, stream=True).raw).convert(\"RGB\")\n",
    "image = Image.open(\"test.png\").convert(\"RGB\")\n",
    "\n",
    "\n",
    "pixel_values = processor(images=image, return_tensors=\"pt\").pixel_values\n",
    "outputs = model(pixel_values)\n",
    "\n",
    "generated_text = processor.batch_decode(outputs.logits)['generated_text']\n",
    "generated_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "814f5c35",
   "metadata": {},
   "source": [
    "[LatexOCR](https://github.com/lukas-blecher/LaTeX-OCR) - Bad, very old implementation that doesn't output correct equations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c15ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from pix2tex.cli import LatexOCR\n",
    "\n",
    "img = Image.open('test.png') #img extracted from HardyCross.pdf with equations\n",
    "model = LatexOCR()\n",
    "print(model(img))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "720a1c50",
   "metadata": {},
   "source": [
    "# Didn't run in MacOS - RunPod used"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59eb3003",
   "metadata": {},
   "source": [
    "[Zerox](https://github.com/getomni-ai/zerox) - not working in RunPod either"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "336657b4-a861-4664-8484-b4a199707a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install py-zerox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb05193",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyzerox import zerox\n",
    "import os\n",
    "import json\n",
    "import asyncio\n",
    "\n",
    "### Model Setup (Use only Vision Models) Refer: https://docs.litellm.ai/docs/providers ###\n",
    "\n",
    "## placeholder for additional model kwargs which might be required for some models\n",
    "kwargs = {}\n",
    "\n",
    "## system prompt to use for the vision model\n",
    "custom_system_prompt = None\n",
    "\n",
    "# to override\n",
    "# custom_system_prompt = \"For the below PDF page, do something..something...\" ## example\n",
    "\n",
    "###################### Example for OpenAI ######################\n",
    "os.environ[\"OPENAI_API_KEY\"] = openai_api_key ## your-api-key\n",
    "\n",
    "# ###################### Example for Azure OpenAI ######################\n",
    "# model = \"azure/gpt-4o-mini\" ## \"azure/<your_deployment_name>\" -> format <provider>/<model>\n",
    "# os.environ[\"AZURE_API_KEY\"] = \"\" # \"your-azure-api-key\"\n",
    "# os.environ[\"AZURE_API_BASE\"] = \"\" # \"https://example-endpoint.openai.azure.com\"\n",
    "# os.environ[\"AZURE_API_VERSION\"] = \"\" # \"2023-05-15\"\n",
    "\n",
    "# ###################### Example for Gemini ######################\n",
    "# model = \"gemini/gpt-4o-mini\" ## \"gemini/<gemini_model>\" -> format <provider>/<model>\n",
    "# os.environ['GEMINI_API_KEY'] = \"\" # your-gemini-api-key\n",
    "\n",
    "# ###################### Example for Anthropic ######################\n",
    "# model=\"claude-3-opus-20240229\"\n",
    "# os.environ[\"ANTHROPIC_API_KEY\"] = \"\" # your-anthropic-api-key\n",
    "\n",
    "# ###################### Vertex ai ######################\n",
    "# model = \"vertex_ai/gemini-1.5-flash-001\" ## \"vertex_ai/<model_name>\" -> format <provider>/<model>\n",
    "# ## GET CREDENTIALS\n",
    "# ## RUN ##\n",
    "# # !gcloud auth application-default login - run this to add vertex credentials to your env\n",
    "# ## OR ##\n",
    "# file_path = 'path/to/vertex_ai_service_account.json'\n",
    "\n",
    "# # Load the JSON file\n",
    "# with open(file_path, 'r') as file:\n",
    "#     vertex_credentials = json.load(file)\n",
    "\n",
    "# # Convert to JSON string\n",
    "# vertex_credentials_json = json.dumps(vertex_credentials)\n",
    "\n",
    "# vertex_credentials=vertex_credentials_json\n",
    "\n",
    "# ## extra args\n",
    "# kwargs = {\"vertex_credentials\": vertex_credentials}\n",
    "\n",
    "###################### For other providers refer: https://docs.litellm.ai/docs/providers ######################\n",
    "\n",
    "# Define main async entrypoint\n",
    "async def main():\n",
    "    file_path = pdf_file ## local filepath and file URL supported\n",
    "\n",
    "    ## process only some pages or all\n",
    "    select_pages = None ## None for all, but could be int or list(int) page numbers (1 indexed)\n",
    "\n",
    "    output_dir = \"./output_test\" ## directory to save the consolidated markdown file\n",
    "    result = await zerox(file_path=file_path, model=openai_small_model, output_dir=output_dir,\n",
    "                        custom_system_prompt=system_prompt,select_pages=select_pages, **kwargs)\n",
    "    return result\n",
    "\n",
    "\n",
    "# run the main function:\n",
    "result = asyncio.run(main())\n",
    "\n",
    "# print markdown result\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78956a78",
   "metadata": {},
   "source": [
    "[Ollama OCR](https://github.com/imanoop7/Ollama-OCR) - install with ```pip install ollama-ocr``` - not great, it's chat with a document not just ocr, gives errors in results and takes 1 min per page with parallel processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16affaa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ollama_ocr import OCRProcessor\n",
    "\n",
    "# Initialize OCR processor\n",
    "ocr = OCRProcessor(model_name='granite3.2-vision', base_url=\"http://host.docker.internal:11434/api/generate\")  # You can use any vision model available on Ollama\n",
    "# you can pass your custom ollama api llama3.2-vision:11b\n",
    "\n",
    "# Process an image\n",
    "result = ocr.process_image(\n",
    "    image_path=pdf_file, # path to your pdf files \"path/to/your/file.pdf\"\n",
    "    format_type=\"markdown\",  # Options: markdown, text, json, structured, key_value\n",
    "    custom_prompt=system_prompt,#\"Extract all text, focusing on dates and names.\", # Optional custom prompt\n",
    "    language=\"English\" # Specify the language of the text (New! 🆕)\n",
    ")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a997366",
   "metadata": {},
   "source": [
    "[X-PLUG](https://github.com/X-PLUG/mPLUG-DocOwl) and official repo [here](https://github.com/X-PLUG/mPLUG-DocOwl) - not good, just vision-LLM on an image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2732e872-ad3f-4231-bff5-2beffa51aa95",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/X-PLUG/mPLUG-Owl.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1754fa2d-f893-4044-a755-9e59ff2d8672",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab9d8572-1d40-403c-a590-a2672de95954",
   "metadata": {},
   "outputs": [],
   "source": [
    "cd mPLUG-Owl/mPLUG-Owl2/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4502d1fa-b8e4-47e4-b215-d35bc2315cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -e ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b30f240-420a-48fe-8377-e1dacedf39e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d511584d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import TextStreamer\n",
    "\n",
    "from mplug_owl2.constants import IMAGE_TOKEN_INDEX, DEFAULT_IMAGE_TOKEN\n",
    "from mplug_owl2.conversation import conv_templates, SeparatorStyle\n",
    "from mplug_owl2.model.builder import load_pretrained_model\n",
    "from mplug_owl2.mm_utils import process_images, tokenizer_image_token, get_model_name_from_path, KeywordsStoppingCriteria\n",
    "\n",
    "image_file = 'test.png' # Image Path of img extracted from HardyCross.pdf with equations\n",
    "model_path = 'MAGAer13/mplug-owl2-llama2-7b'\n",
    "query = \"Describe the image. Give all the equations from it\"\n",
    "\n",
    "model_name = get_model_name_from_path(model_path)\n",
    "tokenizer, model, image_processor, context_len = load_pretrained_model(model_path, None, model_name, load_8bit=True, load_4bit=False, device=\"cuda\")\n",
    "\n",
    "conv = conv_templates[\"mplug_owl2\"].copy()\n",
    "roles = conv.roles\n",
    "\n",
    "image = Image.open(image_file).convert('RGB')\n",
    "max_edge = max(image.size) # We recommand you to resize to squared image for BEST performance.\n",
    "image = image.resize((max_edge, max_edge))\n",
    "\n",
    "image_tensor = process_images([image], image_processor)\n",
    "image_tensor = image_tensor.to(model.device, dtype=torch.float16)\n",
    "\n",
    "inp = DEFAULT_IMAGE_TOKEN + query\n",
    "conv.append_message(conv.roles[0], inp)\n",
    "conv.append_message(conv.roles[1], None)\n",
    "prompt = conv.get_prompt()\n",
    "\n",
    "input_ids = tokenizer_image_token(prompt, tokenizer, IMAGE_TOKEN_INDEX, return_tensors='pt').unsqueeze(0).to(model.device)\n",
    "stop_str = conv.sep2\n",
    "keywords = [stop_str]\n",
    "stopping_criteria = KeywordsStoppingCriteria(keywords, tokenizer, input_ids)\n",
    "streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
    "\n",
    "temperature = 0.7\n",
    "max_new_tokens = 512\n",
    "\n",
    "with torch.inference_mode():\n",
    "    output_ids = model.generate(\n",
    "        input_ids,\n",
    "        images=image_tensor,\n",
    "        do_sample=True,\n",
    "        temperature=temperature,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        streamer=streamer,\n",
    "        use_cache=True,\n",
    "        stopping_criteria=[stopping_criteria])\n",
    "\n",
    "outputs = tokenizer.decode(output_ids[0, input_ids.shape[1]:]).strip()\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b52357d3",
   "metadata": {},
   "source": [
    "[OlmOCR](https://github.com/allenai/olmocr) - Demo seems good, decent in general but not perfect for equations, GPU needed, python=3.11, can't run in MacOS, english only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223ede91-aa49-41f2-bc19-bc8dc663b13a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !apt-get update -y\n",
    "# !apt-get install poppler-utils ttf-mscorefonts-installer msttcorefonts fonts-crosextra-caladea fonts-crosextra-carlito gsfonts lcdf-typetools -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c7ea0cb-86f6-43b4-8ce6-802f7cee8bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/allenai/olmocr.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a0993dd-5cbb-42da-9975-cafd215992fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "cd olmocr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d66129-5c06-46de-802b-2187b333aca5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install -e .[gpu] --find-links https://flashinfer.ai/whl/cu124/torch2.4/flashinfer/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "166e1f42-0f6a-46a7-a993-dad59a357865",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!python -m olmocr.pipeline /workspace --pdfs /workspace/HardyCross.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af1321f",
   "metadata": {},
   "source": [
    "[GOT OCR](https://huggingface.co/stepfun-ai/GOT-OCR2_0) - not good, version from [Git](https://github.com/Ucas-HaoranWei/GOT-OCR2.0.git) doesn't work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf92eb9-485c-4950-95a9-110b656a7414",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torch==2.0.1 torchvision==0.15.2 transformers==4.37.2 tiktoken==0.6.0 verovio==4.3.1 accelerate==0.28.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0945140-021d-4ecb-8d3f-7f7b6b4108bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('ucaslcl/GOT-OCR2_0', trust_remote_code=True)\n",
    "model = AutoModel.from_pretrained('ucaslcl/GOT-OCR2_0', trust_remote_code=True, low_cpu_mem_usage=True, device_map='cuda', use_safetensors=True, pad_token_id=tokenizer.eos_token_id)\n",
    "model = model.eval().cuda()\n",
    "\n",
    "\n",
    "# input your test image\n",
    "image_file = \"/workspace/test.png\" #img with the first page from HardyCross.pdf with equations\n",
    "\n",
    "# plain texts OCR\n",
    "res = model.chat(tokenizer, image_file, ocr_type='ocr')\n",
    "\n",
    "# format texts OCR:\n",
    "# res = model.chat(tokenizer, image_file, ocr_type='format')\n",
    "\n",
    "# fine-grained OCR:\n",
    "# res = model.chat(tokenizer, image_file, ocr_type='ocr', ocr_box='')\n",
    "# res = model.chat(tokenizer, image_file, ocr_type='format', ocr_box='')\n",
    "# res = model.chat(tokenizer, image_file, ocr_type='ocr', ocr_color='')\n",
    "# res = model.chat(tokenizer, image_file, ocr_type='format', ocr_color='')\n",
    "\n",
    "# multi-crop OCR:\n",
    "# res = model.chat_crop(tokenizer, image_file, ocr_type='ocr')\n",
    "# res = model.chat_crop(tokenizer, image_file, ocr_type='format')\n",
    "\n",
    "# render the formatted OCR results:\n",
    "# res = model.chat(tokenizer, image_file, ocr_type='format', render=True, save_render_file = './demo.html')\n",
    "\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39eeb2b9",
   "metadata": {},
   "source": [
    "[Nougat OCR](https://facebookresearch.github.io/nougat/) - old implementation and not good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09560101-082b-477f-9f4a-33d09fe2650b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install nougat-ocr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae0c5fe1-eabc-4210-ae35-bc441f9a8b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install albumentations==1.0.0 transformers==4.38.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f515e2-f706-4dc2-be8a-29486d30656a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nougat /workspace/HardyCross.pdf -o /workspace"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "120e9c69",
   "metadata": {},
   "source": [
    "[MegaParse](https://github.com/QuivrHQ/MegaParse) - Decent but needs an extra LLM round to convert to proper Markdown equations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f342bfcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from megaparse.parser.megaparse_vision import MegaParseVision\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "# Load environment variables from .env file\n",
    "load_dotenv(dotenv_path=os.getcwd()+\"/env\")\n",
    "\n",
    "model = ChatOpenAI(model=openai_model, api_key=openai_api_key)  # type: ignore\n",
    "parser = MegaParseVision(model=model)\n",
    "response = parser.convert(\"./HardyCross.pdf\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e7e3470",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "display(Markdown(response.content[0].text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5703ad47",
   "metadata": {},
   "source": [
    "# Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fba805a",
   "metadata": {},
   "source": [
    "[Smolagents](https://huggingface.co/docs/smolagents/en/tutorials/secure_code_execution) - Excellent code agent (use cheap model since uses a lot of tokens, even for simple tasks due to many iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a32db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install smolagents\n",
    "# !pip install torch\n",
    "# !pip install colpali_engine\n",
    "# !pip install 'smolagents[e2b]'\n",
    "# !pip install openpyxl==3.1.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffcacc3c",
   "metadata": {},
   "source": [
    "Run smolagents in an isolated environment with secure code execution - For Models check [here](https://huggingface.co/docs/smolagents/en/reference/models)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c5591b1",
   "metadata": {},
   "source": [
    "| Feature            | `executor_type=\"e2b\"`                     | `Sandbox()` (local)               |\n",
    "|--------------------|-------------------------------------------|-----------------------------------|\n",
    "| Execution Location | Cloud container                          | Local Python environment          |\n",
    "| Isolation          | High                                      | Medium (unless manually hardened) |\n",
    "| Setup Overhead     | Needs E2B setup & keys                    | None                              |\n",
    "| Latency            | Slightly higher due to remote execution  | Low                               |\n",
    "| API Credentials    | Must be injected                          | Local env accessible              |\n",
    "| Scaling            | Cloud-native scaling                      | Limited by local resources        |\n",
    "| Best For           | Untrusted code, scalable production       | Local dev, fast prototyping       |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc44c7cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "# Load environment variables from .env file\n",
    "load_dotenv(dotenv_path=os.getcwd()+\"/env\")\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY_DRACO')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d081a95",
   "metadata": {},
   "source": [
    "The approach below prints all the steps/planning of the model and errors - Streaming responses, sometimes seems it needs less tokens for the same task compared to sandbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a97ada8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #This for using together AI - needs credits\n",
    "# from smolagents import InferenceClientModel, CodeAgent\n",
    "# agent = CodeAgent(model=InferenceClientModel(), tools=[], executor_type=\"e2b\")\n",
    "# agent.run(\"Can you give me the 100th Fibonacci number?\")\n",
    "\n",
    "import base64, openpyxl\n",
    "from io import StringIO\n",
    "from smolagents import CodeAgent\n",
    "from smolagents import OpenAIServerModel\n",
    "\n",
    "model = OpenAIServerModel(\n",
    "    model_id=\"gpt-4o-mini\",\n",
    "    api_base=\"https://api.openai.com/v1\",\n",
    "    api_key=openai_api_key,\n",
    ")\n",
    "\n",
    "agent = CodeAgent(\n",
    "    model=model,  # or \"gpt-3.5-turbo\"\n",
    "    tools=[],\n",
    "    executor_type=\"e2b\"\n",
    ")\n",
    "\n",
    "b64_data = agent.run(\n",
    "    # \"\"\"Create a DataFrame with one column and 60 rows containing the first 60 Fibonacci numbers. \n",
    "    # Return the file contents as base64 so I can download it.\"\"\"\n",
    "    \"\"\"\n",
    "    Using Python code and relevant tools (e.g., networkx), solve the following problem: \n",
    "    \n",
    "    DESCRIPTION: Consider a pipe network consisting of four junctions arranged in a diamond shape:\n",
    "    junction A is on the left, junction B is at the top, junction C is at the bottom, and junction D is at the right. There's also a vertical pipe connecting junction B (top) directly \n",
    "    with junction C (bottom), dividing the network into two loops. The flows at the junctions are as follows: junction A has an inflow of 100 L/s, junction B has an outflow of 25 L/s, \n",
    "    and junction D has an outflow of 75 L/s. The pipes, their resistances (K values), and initial guesses for flow (Q) are given as follows: Pipe AB connects junctions A and B, \n",
    "    has K = 1, and an initial assumed flow of 60 L/s directed from A to B. Pipe BC connects junctions B and C (vertical pipe), has K = 3, and an initial assumed flow of 15 L/s directed\n",
    "    from B to C. Pipe AC connects junctions A and C, has K = 2, and an initial assumed flow of 40 L/s directed from A to C. Pipe BD connects junctions B and D, has K = 2, and an initial \n",
    "    assumed flow of 20 L/s directed from B to D. Pipe CD connects junctions C and D, has K = 1, and an initial assumed flow of 55 L/s directed from C to D. Two loops are defined for the\n",
    "    Hardy Cross method. Loop 1 is the left loop formed by junctions A-B-C-A, with clockwise flow direction taken as positive. Loop 2 is the right loop formed by junctions B-D-C-B, \n",
    "    with clockwise flow direction also taken as positive.\n",
    "\n",
    "    TASK: Your task is to perform one iteration of the Hardy Cross method calculations for both loops, and report the corrected flows in each pipe after one iteration, clearly stating\n",
    "    the direction and magnitude of the flows. OUTPUT FORMAT: The output should be formatted as a JSON object, where each key is a pipe ID and each value is the corrected flow in L/s after \n",
    "    one iteration. The sign of the flow indicates the direction, where a positive value means the flow is in the same direction as initially assumed, while a negative value means the flow \n",
    "    has reversed direction relative to the initial assumption. Return only the JSON object without any other information.\n",
    "    \n",
    "    \"\"\"\n",
    ") # assuming it returns the base64 string\n",
    "\n",
    "# decoded = base64.b64decode(b64_data).decode('utf-8')\n",
    "# with open('fibonacci.csv', 'w', encoding='utf-8') as f:\n",
    "#     f.write(decoded)\n",
    "\n",
    "# lines = decoded.strip().splitlines()\n",
    "# wb = openpyxl.Workbook()\n",
    "# ws = wb.active\n",
    "# for i, line in enumerate(lines, start=1):\n",
    "#     ws.cell(row=i, column=1).value = line\n",
    "\n",
    "# wb.save('fibonacci.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b5511d",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval(b64_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a12fb9f8",
   "metadata": {},
   "source": [
    "The approach below is better since we have have in a variable the output, and in another one the execution steps/planning of the model - Only prints at the end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0488d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "from e2b_code_interpreter import Sandbox\n",
    "import os\n",
    "\n",
    "# Create the sandbox\n",
    "sandbox = Sandbox()\n",
    "\n",
    "# Install required packages\n",
    "sandbox.commands.run(\"pip install smolagents\")\n",
    "sandbox.commands.run(\"pip install 'smolagents[openai]'\") #to use openai model\n",
    "\n",
    "def run_code_raise_errors(sandbox, code: str, verbose: bool = False) -> str:\n",
    "    execution = sandbox.run_code(\n",
    "        code,\n",
    "        envs={#'HF_TOKEN': os.getenv('HF_TOKEN'),\n",
    "              'OPENAI_API_KEY': openai_api_key}\n",
    "    )\n",
    "    if execution.error:\n",
    "        execution_logs = \"\\n\".join([str(log) for log in execution.logs.stdout])\n",
    "        logs = execution_logs\n",
    "        logs += execution.error.traceback\n",
    "        raise ValueError(logs)\n",
    "    return \"\\n\".join([str(log) for log in execution.logs.stdout]), execution\n",
    "\n",
    "# #For inference with an open-source model using together AI, replace below with:\n",
    "# from smolagents import CodeAgent, InferenceClientModel\n",
    "\n",
    "# # Initialize the agents\n",
    "# agent = CodeAgent(\n",
    "#     model=InferenceClientModel(token=os.getenv(\"HF_TOKEN\"), provider=\"together\"),\n",
    "#     tools=[],\n",
    "#     name=\"coder_agent\",\n",
    "#     description=\"This agent takes care of your difficult algorithmic problems using code.\"\n",
    "# )\n",
    "\n",
    "# manager_agent = CodeAgent(\n",
    "#     model=InferenceClientModel(token=os.getenv(\"HF_TOKEN\"), provider=\"together\"),\n",
    "#     tools=[],\n",
    "#     managed_agents=[agent],\n",
    "# )\n",
    "\n",
    "# Define your agent application\n",
    "agent_code = \"\"\"\n",
    "import os\n",
    "from smolagents import CodeAgent\n",
    "from smolagents import OpenAIServerModel\n",
    "\n",
    "model = OpenAIServerModel(\n",
    "    model_id=\"gpt-4o-mini\",\n",
    "    api_base=\"https://api.openai.com/v1\",\n",
    "    api_key=os.getenv('OPENAI_API_KEY'),\n",
    ")\n",
    "\n",
    "# Initialize the agents\n",
    "agent = CodeAgent(\n",
    "    model=model,\n",
    "    tools=[],\n",
    "    name=\"coder_agent\",\n",
    "    description=\"This agent takes care of your difficult algorithmic problems using code.\",\n",
    "    additional_authorized_imports=['json']\n",
    ")\n",
    "\n",
    "manager_agent = CodeAgent(\n",
    "    model=model,\n",
    "    tools=[],\n",
    "    managed_agents=[agent],\n",
    "    additional_authorized_imports=['json']\n",
    ")\n",
    "\n",
    "# Run the agent\n",
    "response = manager_agent.run(\" Using Python code and relevant tools (e.g., networkx), solve the following problem: \\\n",
    "    DESCRIPTION: Consider a pipe network consisting of four junctions arranged in a diamond shape: \\\n",
    "    junction A is on the left, junction B is at the top, junction C is at the bottom, and junction D is at the right. There's also a vertical pipe connecting junction B (top) directly \\\n",
    "    with junction C (bottom), dividing the network into two loops. The flows at the junctions are as follows: junction A has an inflow of 100 L/s, junction B has an outflow of 25 L/s, \\\n",
    "    and junction D has an outflow of 75 L/s. The pipes, their resistances (K values), and initial guesses for flow (Q) are given as follows: Pipe AB connects junctions A and B, \\\n",
    "    has K = 1, and an initial assumed flow of 60 L/s directed from A to B. Pipe BC connects junctions B and C (vertical pipe), has K = 3, and an initial assumed flow of 15 L/s directed \\\n",
    "    from B to C. Pipe AC connects junctions A and C, has K = 2, and an initial assumed flow of 40 L/s directed from A to C. Pipe BD connects junctions B and D, has K = 2, and an initial \\\n",
    "    assumed flow of 20 L/s directed from B to D. Pipe CD connects junctions C and D, has K = 1, and an initial assumed flow of 55 L/s directed from C to D. Two loops are defined for the \\\n",
    "    Hardy Cross method. Loop 1 is the left loop formed by junctions A-B-C-A, with clockwise flow direction taken as positive. Loop 2 is the right loop formed by junctions B-D-C-B, \\\n",
    "    with clockwise flow direction also taken as positive. \\\n",
    "    TASK: Your task is to perform one iteration of the Hardy Cross method calculations for both loops, and report the corrected flows in each pipe after one iteration, clearly stating \\\n",
    "    the direction and magnitude of the flows. OUTPUT FORMAT: The output should be formatted as a JSON object, where each key is a pipe ID and each value is the corrected flow in L/s after \\\n",
    "    one iteration. The sign of the flow indicates the direction, where a positive value means the flow is in the same direction as initially assumed, while a negative value means the flow \\\n",
    "    has reversed direction relative to the initial assumption. Return only the JSON object without any other information. \")\n",
    "print(response)\n",
    "\"\"\" #'Create a DataFrame with one column and 60 rows containing the first 60 Fibonacci numbers. Return the file contents as base64 so I can download it.'\n",
    "\n",
    "# Run the agent code in the sandbox\n",
    "execution_logs, execution_result = run_code_raise_errors(sandbox, agent_code)\n",
    "print(\"Logs:\",execution_logs)\n",
    "print(\"Results of execution:\", execution_result)\n",
    "\n",
    "# decoded = base64.b64decode(execution_logs).decode('utf-8')\n",
    "# with open('fibonacci.csv', 'w', encoding='utf-8') as f:\n",
    "#     f.write(decoded)\n",
    "\n",
    "# lines = decoded.strip().splitlines()\n",
    "# wb = openpyxl.Workbook()\n",
    "# ws = wb.active\n",
    "# for i, line in enumerate(lines, start=1):\n",
    "#     ws.cell(row=i, column=1).value = line\n",
    "\n",
    "# wb.save('fibonacci.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d8a3d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval(execution_logs.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9960a81",
   "metadata": {},
   "source": [
    "Enable tool usage in [smolagents](https://colab.research.google.com/drive/1iSjTCsIggeevlQojsYdLmzTKZ7og7rva?usp=sharing) - need to ```pip install 'smolagents[litellm]'```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3348df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from smolagents import CodeAgent, LiteLLMModel #DuckDuckGoSearchTool, \n",
    "\n",
    "model = LiteLLMModel(model_id=openai_model, api_key=openai_api_key)\n",
    "\n",
    "agent = CodeAgent(tools=[], model=model, add_base_tools=True) #add_base_tool adds by default python code interpreter to tool list\n",
    "\n",
    "agent.run(\n",
    "    \"Could you give me the 118th number in the Fibonacci sequence?\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5178ce5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = CodeAgent(tools=[], model=model) #only few imports by default like math. Other libraries should be passed as argument additional_authorized_imports=['requests', 'bs4']\n",
    "agent.run(\"Could you get me the title of the page at url 'https://huggingface.co/blog'?\") #Might get different outputs every time we run this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ec279f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LiteLLMModel(model_id=openai_small_model, api_key=openai_api_key, temperature=0, seed=42) #Still not reproducible results\n",
    "agent = CodeAgent(tools=[], model=model, max_steps=5)  # Steps set to 5 to avoid charging more\n",
    "for i in range(5):\n",
    "    agent.run(\"Could you get me the title of the page at url 'https://huggingface.co/blog'? Use requests to fetch data from it\") \n",
    "#Might get different outputs every time we run this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5207e429",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = CodeAgent(tools=[], model=model, additional_authorized_imports=['requests', 'bs4']) #These authorize the use of these libraries\n",
    "agent.run(\"Could you get me the title of the page at url 'https://huggingface.co/blog'? Use requests to fetch data from it\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e3cb068",
   "metadata": {},
   "source": [
    "Below works even without python interpreter!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875ba4a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.run(\"How many seconds would it take for a leopard at full speed to run through Pont des Arts?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b05e2d1",
   "metadata": {},
   "source": [
    "PythonInterpreterTool is enabled by default since we set ```add_base_tools=True``` - below actually same as above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b1cddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(agent.system_prompt_template) #To see what is the prompt template used\n",
    "from smolagents import ToolCallingAgent, PythonInterpreterTool\n",
    "\n",
    "# modified_prompt = \"....\"\n",
    "\n",
    "agent = ToolCallingAgent(tools=[PythonInterpreterTool()], model=model)#, system_prompt=modified_prompt)\n",
    "\n",
    "agent.run(\"How many seconds would it take for a leopard at full speed to run through Pont des Arts?\")\n",
    "\n",
    "print(agent.logs) #To see full output\n",
    "# agent.write_inner_memory_from_logs()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3669c5a4",
   "metadata": {},
   "source": [
    "Create our custom tool within smolagents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae508f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import tool\n",
    "\n",
    "@tool\n",
    "def do_sth_tool(task: str) -> str:\n",
    "    \"\"\"\n",
    "    This is a description of a tool that does sth.\n",
    "\n",
    "    Args:\n",
    "        task: The task for which\n",
    "    \"\"\"\n",
    "\n",
    "    return \n",
    "\n",
    "agent = CodeAgent(tools=[do_sth_tool], model=model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9edf7822",
   "metadata": {},
   "source": [
    "Import tool from HF space - ```pip install 'smolagents[gradio]'```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b43528af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from smolagents import (\n",
    "    load_tool,\n",
    "    CodeAgent,\n",
    "    # HfApiModel,\n",
    "    GradioUI\n",
    ")\n",
    "\n",
    "# Import tool from Hub\n",
    "image_generation_tool = load_tool(\"m-ric/text-to-image\", trust_remote_code=True)\n",
    "\n",
    "# model = HfApiModel(model_id)\n",
    "\n",
    "# Initialize the agent with the image generation tool\n",
    "agent = CodeAgent(tools=[image_generation_tool], model=model)\n",
    "\n",
    "GradioUI(agent).launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a67c5ba",
   "metadata": {},
   "source": [
    "Answer more complicated questions by enabling web search (cannot use ManagedAgent anymore) - ```pip install 'smolagents[litellm]'```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "436ac0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from smolagents import DuckDuckGoSearchTool \n",
    "from smolagents import CodeAgent, LiteLLMModel\n",
    "\n",
    "model = LiteLLMModel(model_id=openai_model, api_key=openai_api_key)\n",
    "web_agent = CodeAgent(tools=[DuckDuckGoSearchTool()], model=model)\n",
    "\n",
    "answer = web_agent.run(\"Which dynasty was ruling China at the time of the fall of Constantinople?.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c6ff6d",
   "metadata": {},
   "source": [
    "Call model from [providers supported in HF](https://huggingface.co/docs/inference-providers/index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e145ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from smolagents import InferenceClientModel\n",
    "\n",
    "# model = InferenceClientModel(\n",
    "#     model_id=\"deepseek-ai/DeepSeek-R1\",\n",
    "#     provider=\"together\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a88f6790",
   "metadata": {},
   "source": [
    "OpenAI compatible server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5234c0ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from smolagents import OpenAIServerModel\n",
    "\n",
    "# model = OpenAIServerModel(\n",
    "#     model_id=\"deepseek-ai/DeepSeek-R1\",\n",
    "#     api_base=\"https://api.together.xyz/v1/\", # Leave this blank to query OpenAI servers.\n",
    "#     api_key=os.environ[\"TOGETHER_API_KEY_DRACO\"], # Switch to the API key for the server you're targeting.\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd93156",
   "metadata": {},
   "source": [
    "Local model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087898b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from smolagents import TransformersModel\n",
    "\n",
    "# model = TransformersModel(\n",
    "#     model_id=\"Qwen/Qwen2.5-Coder-32B-Instruct\",\n",
    "#     max_new_tokens=4096,\n",
    "#     device_map=\"auto\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "047f85ba",
   "metadata": {},
   "source": [
    "[Google adk](https://github.com/google/adk-python) - Doesn't produce full answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5d9b8ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "# Load environment variables from .env file\n",
    "load_dotenv(dotenv_path=os.getcwd()+\"/env\")\n",
    "\n",
    "os.environ[\"GOOGLE_API_KEY\"] = os.getenv(\"GEMINI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2ca57d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.adk.agents import Agent\n",
    "from google.adk.tools import google_search, built_in_code_execution\n",
    "\n",
    "\n",
    "root_agent = Agent(\n",
    "    name=\"search_assistant\",\n",
    "    model=\"gemini-2.0-flash\", # Or your preferred Gemini model\n",
    "    instruction=\"You are a helpful assistant.\",#, Answer user questions using Google Search when needed.\",\n",
    "    # description=\"An assistant that can search the web.\",\n",
    "    tools=[built_in_code_execution]#google_search]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0476cf77",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.adk.sessions import InMemorySessionService\n",
    "from google.adk.runners import Runner\n",
    "\n",
    "APP_NAME=\"customer_support_agent\"\n",
    "USER_ID=\"user1234\"\n",
    "SESSION_ID=\"1234\"\n",
    "\n",
    "# Session and Runner\n",
    "session_service = InMemorySessionService()\n",
    "session = session_service.create_session(app_name=APP_NAME, user_id=USER_ID, session_id=SESSION_ID)\n",
    "runner = Runner(agent=root_agent, app_name=APP_NAME, session_service=session_service)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43da5d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.genai import types\n",
    "\n",
    "# Agent Interaction\n",
    "def call_agent(query):\n",
    "    content = types.Content(role='user', parts=[types.Part(text=query)])\n",
    "    events = runner.run(user_id=USER_ID, session_id=SESSION_ID, new_message=content)\n",
    "\n",
    "    for event in events:\n",
    "        if event.is_final_response():\n",
    "            final_response = event.content.parts[0].text\n",
    "            print(\"Agent Response: \", final_response)\n",
    "\n",
    "call_agent(\"Calculate the 12 fibonacci number?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e18ac149",
   "metadata": {},
   "source": [
    "# Visual RAG - Only use with GPU/RunPod"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "395ae06c",
   "metadata": {},
   "source": [
    "[Visual RAG with smolagents](https://huggingface.co/blog/paultltc/deepsearch-using-visual-rag) - Didn't work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "12500b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install accelerate wheel pdf2image\n",
    "#!pip install flash-attn --no-build-isolation\n",
    "\n",
    "#!apt update\n",
    "#!apt install poppler-utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "103f17b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "# Load environment variables from .env file\n",
    "load_dotenv(dotenv_path=os.getcwd()+\"/env\")\n",
    "\n",
    "# Get the OpenAI API key\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY_DRACO')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17658d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from smolagents import load_tool\n",
    "from flash_attn import flash_attn_varlen_func\n",
    "# Load the visual RAG tool\n",
    "visual_rag_tool = load_tool(\n",
    "    \"vidore/visual-rag-tool\",\n",
    "    trust_remote_code=True,\n",
    "    api_key=openai_api_key,\n",
    ")\n",
    "\n",
    "# Index the PDF document\n",
    "visual_rag_tool.index([pdf_file])\n",
    "\n",
    "# Query the tool\n",
    "visual_rag_tool(\"What is the final DQ equation?\", k=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8fabfda",
   "metadata": {},
   "source": [
    "[Cohere Visual RAG](https://colab.research.google.com/drive/1JwZ_nWhBUFbrzJnHKmyd0qKJ3gVt5lCe?usp=sharing#scrollTo=tzTg5U8Okfjl) - Extract information from images ```pip install cohere```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c81f8f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"/workspace\") #Use if in RunPod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e868f281",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from google import genai #pip install google-genai\n",
    "import cohere\n",
    "import numpy as np\n",
    "import PIL\n",
    "import io\n",
    "import base64\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv(dotenv_path=os.getcwd()+\"/env\")\n",
    "cohere_api_key = os.getenv('COHERE_API_KEY')\n",
    "gemini_api_key = os.getenv(\"GEMINI_API_KEY\")  #Replace with your Gemini API key\n",
    "\n",
    "client = genai.Client(api_key=gemini_api_key)\n",
    "co = cohere.ClientV2(api_key=cohere_api_key)\n",
    "\n",
    "gemini_model=\"gemini-2.5-flash-preview-04-17\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f716a94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from IPython.display import HTML, display\n",
    "\n",
    "# def set_css():\n",
    "#   display(HTML('''\n",
    "#   <style>\n",
    "#     pre {\n",
    "#         white-space: pre-wrap;\n",
    "#     }\n",
    "#   </style>\n",
    "#   '''))\n",
    "# get_ipython().events.register('pre_run_cell', set_css)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ba5707ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Some helper functions to resize images and to convert them to base64 format\n",
    "max_pixels = 1568*1568  #Max resolution for images\n",
    "\n",
    "# Resize too large images\n",
    "def resize_image(pil_image):\n",
    "    org_width, org_height = pil_image.size\n",
    "\n",
    "    # Resize image if too large\n",
    "    if org_width * org_height > max_pixels:\n",
    "        scale_factor = (max_pixels / (org_width * org_height)) ** 0.5\n",
    "        new_width = int(org_width * scale_factor)\n",
    "        new_height = int(org_height * scale_factor)\n",
    "        pil_image.thumbnail((new_width, new_height))\n",
    "\n",
    "# Convert images to a base64 string before sending it to the API\n",
    "def base64_from_image(img_path):\n",
    "    pil_image = PIL.Image.open(img_path)\n",
    "    img_format = pil_image.format if pil_image.format else \"PNG\"\n",
    "\n",
    "    resize_image(pil_image)\n",
    "\n",
    "    with io.BytesIO() as img_buffer:\n",
    "        pil_image.save(img_buffer, format=img_format)\n",
    "        img_buffer.seek(0)\n",
    "        img_data = f\"data:image/{img_format.lower()};base64,\"+base64.b64encode(img_buffer.read()).decode(\"utf-8\")\n",
    "\n",
    "    return img_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc6e6878",
   "metadata": {},
   "outputs": [],
   "source": [
    "# images = { #If we want to download images from the web\n",
    "#     \"tesla.png\": \"https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbef936e6-3efa-43b3-88d7-7ec620cdb33b_2744x1539.png\",\n",
    "#     \"netflix.png\": \"https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F23bd84c9-5b62-4526-b467-3088e27e4193_2744x1539.png\",\n",
    "#     \"nike.png\": \"https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa5cd33ba-ae1a-42a8-a254-d85e690d9870_2741x1541.png\",\n",
    "#     \"google.png\": \"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F395dd3b9-b38e-4d1f-91bc-d37b642ee920_2741x1541.png\",\n",
    "#     \"accenture.png\": \"https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F08b2227c-7dc8-49f7-b3c5-13cab5443ba6_2741x1541.png\",\n",
    "#     \"tecent.png\": \"https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0ec8448c-c4d1-4aab-a8e9-2ddebe0c95fd_2741x1541.png\"\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0892685b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the images and compute an embedding for each image\n",
    "img_folder = \"img\"\n",
    "\n",
    "img_paths = []\n",
    "doc_embeddings = []\n",
    "for img_path in os.listdir(img_folder):\n",
    "# for name, url in tqdm.tqdm(images.items()):\n",
    "    # img_path = os.path.join(img_folder, name)\n",
    "    img_paths.append(os.path.join(img_folder, img_path))\n",
    "\n",
    "    # # Download the image\n",
    "    # if not os.path.exists(img_path):\n",
    "    #     response = requests.get(url)\n",
    "    #     response.raise_for_status()\n",
    "\n",
    "    #     with open(img_path, \"wb\") as fOut:\n",
    "    #         fOut.write(response.content)\n",
    "\n",
    "    # Get the base64 representation of the image\n",
    "    api_input_document = {\n",
    "        \"content\": [\n",
    "            {\"type\": \"image\", \"image\": base64_from_image(os.path.join(img_folder, img_path))},\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    # Call the Embed v4.0 model with the image information\n",
    "    api_response = co.embed(\n",
    "        model=\"embed-v4.0\",\n",
    "        input_type=\"search_document\",\n",
    "        embedding_types=[\"float\"],\n",
    "        inputs=[api_input_document],\n",
    "    )\n",
    "\n",
    "    # Append the embedding to our doc_embeddings list\n",
    "    emb = np.asarray(api_response.embeddings.float[0])\n",
    "    doc_embeddings.append(emb)\n",
    "\n",
    "doc_embeddings = np.vstack(doc_embeddings)\n",
    "print(\"\\n\\nEmbeddings shape:\", doc_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7a7e7091",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search allows us to find relevant images for a given question using Cohere Embed v4\n",
    "def search(question, max_img_size=800):\n",
    "    # Compute the embedding for the query\n",
    "    api_response = co.embed(\n",
    "        model=\"embed-v4.0\",\n",
    "        input_type=\"search_query\",\n",
    "        embedding_types=[\"float\"],\n",
    "        texts=[question],\n",
    "    )\n",
    "\n",
    "    query_emb = np.asarray(api_response.embeddings.float[0])\n",
    "\n",
    "    # Compute cosine similarities\n",
    "    cos_sim_scores = np.dot(query_emb, doc_embeddings.T)\n",
    "\n",
    "    # Get the most relevant image\n",
    "    top_idx = np.argmax(cos_sim_scores)\n",
    "\n",
    "    # Show the images\n",
    "    print(\"Question:\", question)\n",
    "\n",
    "    hit_img_path = img_paths[top_idx]\n",
    "\n",
    "    print(\"Most relevant image:\", hit_img_path)\n",
    "    image = PIL.Image.open(hit_img_path)\n",
    "    max_size = (max_img_size, max_img_size)  # Adjust the size as needed\n",
    "    image.thumbnail(max_size)\n",
    "    display(image)\n",
    "    return hit_img_path\n",
    "\n",
    "# Answer the question based on the information from the image\n",
    "# Here we use Gemini 2.5 as powerful Vision-LLM\n",
    "def answer(question, img_path):\n",
    "    prompt = [f\"\"\"Answer the question based on the following image.\n",
    "Don't use markdown.\n",
    "Please provide enough context for your answer.\n",
    "\n",
    "Question: {question}\"\"\", PIL.Image.open(img_path)]\n",
    "\n",
    "    response = client.models.generate_content(\n",
    "        model=gemini_model,\n",
    "        contents=prompt\n",
    "    )\n",
    "\n",
    "    answer = response.text\n",
    "    print(\"LLM Answer:\", answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9464198",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the query\n",
    "question = \"What is the Dq final equation?\" #\"What is the counter-clockwise head loss in a pipe?\"\n",
    "\n",
    "# Search for the most relevant image\n",
    "top_image_path = search(question)\n",
    "\n",
    "# Use the image to answer the query\n",
    "answer(question, top_image_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89039b0c",
   "metadata": {},
   "source": [
    "Colpali and Gemini Flash [(same as above)](https://colab.research.google.com/drive/1JwZ_nWhBUFbrzJnHKmyd0qKJ3gVt5lCe?usp=sharing#scrollTo=blAg0HiXA3ae). Original notebook in [here](https://colab.research.google.com/github/merveenoyan/smol-vision/blob/main/ColPali_%2B_Qwen2_VL.ipynb).\n",
    "\n",
    "<!-- [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/merveenoyan/smol-vision/blob/main/ColPali_%2B_Qwen2_VL.ipynb) -->\n",
    "\n",
    "\n",
    "[ColPali](https://huggingface.co/blog/manu/colpali) is a multimodal retriever that natively handles images and processes and encodes image patches to be compatible with text, thus removing need to do OCR, or image captioning.\n",
    "\n",
    "![ColPali](https://cdn-uploads.huggingface.co/production/uploads/60f2e021adf471cbdf8bb660/La8vRJ_dtobqs6WQGKTzB.png)\n",
    "\n",
    "[Byaldi](https://github.com/AnswerDotAI/byaldi) is a new library by answer.ai to easily use ColPali."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab3eaad",
   "metadata": {},
   "source": [
    "Install with the following (deactivate autoawq and transformers from initial installation and add pdf file and img folder with pdf images):\n",
    "```bash\n",
    "pip install --upgrade byaldi\n",
    "\n",
    "apt-get update\n",
    "\n",
    "apt-get install -y poppler-utils  # not working in macOS\n",
    "\n",
    "pip install -q pdf2image git+https://github.com/huggingface/transformers.git qwen-vl-utils  # needed to get Qwen - \n",
    "\n",
    "pip uninstall flash-attn -y\n",
    "\n",
    "pip install ninja\n",
    "\n",
    "pip install packaging\n",
    "\n",
    "git clone https://github.com/Dao-AILab/flash-attention\n",
    "\n",
    "cd flash-attention\n",
    "\n",
    "pip install .\n",
    "\n",
    "pip uninstall torch torchvision -y\n",
    "\n",
    "pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
    "\n",
    "pip install ipywidgets\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f011f3c3",
   "metadata": {},
   "source": [
    "We should initialize `RAGMultiModalModel` object with a ColPali model from Hugging Face. By default this model uses GPU but we are going to have Qwen2-VL in the same GPU so we are loading this in CPU for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7aaecdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import base64\n",
    "import io\n",
    "os.chdir(\"/workspace\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "381a1230",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_file=\"HardyCross.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2599d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from byaldi import RAGMultiModalModel\n",
    "\n",
    "RAG = RAGMultiModalModel.from_pretrained(\"vidore/colpali-v1.2\", verbose=1) #First time takes ~70secs to download\n",
    "#can also use device=\"cpu\"\n",
    "\n",
    "RAG.index( #index our document using RAG\n",
    "    input_path=pdf_file,#\"./img/\", #or pdf path\n",
    "    index_name=\"attention\", # index will be saved at index_root/index_name/\n",
    "    store_collection_with_index=True,\n",
    "    overwrite=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a74d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "question = text_query=\"What is the Dq final equation?\" # \"What is the counter-clockwise head loss in a pipe?\"\n",
    "\n",
    "results = RAG.search(question, k=1) #retrieve top 1 image\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c001d66e",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_bytes = base64.b64decode(results[0].base64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368a2310",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_results(question):\n",
    "  results = RAG.search(question, k=1)\n",
    "  image_bytes = base64.b64decode(results[0].base64)\n",
    "  # Convert bytes to image using Pillow\n",
    "  image = Image.open(io.BytesIO(image_bytes))\n",
    "\n",
    "  # Display the image\n",
    "  max_size = (800, 800)\n",
    "  image.thumbnail(max_size)\n",
    "  display(image)\n",
    "\n",
    "show_results(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f76dc5",
   "metadata": {},
   "source": [
    "After indexing and retrieving data, we will use [Qwen2-VL-7B](https://huggingface.co/Qwen/Qwen2-VL-7B-Instruct) to build a RAG pipeline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c06cee3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name=\"Qwen/Qwen2-VL-2B-Instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08405331",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Qwen2VLForConditionalGeneration, AutoProcessor\n",
    "from qwen_vl_utils import process_vision_info\n",
    "import torch\n",
    "\n",
    "model = Qwen2VLForConditionalGeneration.from_pretrained(model_name,\n",
    "                                                        trust_remote_code=True, torch_dtype=torch.bfloat16).cuda().eval() #first time takes ~45 secs to download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4c7b83f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdf2image import convert_from_path\n",
    "images = convert_from_path(\"/workspace/HardyCross.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5c78eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = AutoProcessor.from_pretrained(model_name, trust_remote_code=True)\n",
    "\n",
    "image_index = results[0][\"page_num\"] - 1\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"type\": \"image\",\n",
    "                \"image\": images[image_index],\n",
    "            },\n",
    "            {\"type\": \"text\", \"text\": question},\n",
    "        ],\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b6f401be",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = processor.apply_chat_template(\n",
    "    messages, tokenize=False, add_generation_prompt=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3a50f59c",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_inputs, video_inputs = process_vision_info(messages)\n",
    "inputs = processor(\n",
    "    text=[text],\n",
    "    images=image_inputs,\n",
    "    videos=video_inputs,\n",
    "    padding=True,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "inputs = inputs.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fff5579",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_ids = model.generate(**inputs, max_new_tokens=50)\n",
    "generated_ids_trimmed = [\n",
    "    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "]\n",
    "output_text = processor.batch_decode(\n",
    "    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    ")\n",
    "\n",
    "print(output_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ocr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
